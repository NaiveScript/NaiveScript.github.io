---
title: "基于线性关联单元的深度网络机器翻译模型"
author: "H2o"
date: "August 6, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

## Deep NMT with Linear Associate Unit    
+ 引言与摘要：虽然深度学习得益于模型非线性近似能力在很多领域都取得极大的成功，但是与深度相关的话题一直也是学术界研究的重点．从sigmoid到Relu,从VGG到ResNet,相关研究也在围绕着如何训练和学习模型中展开．当网络已经具备抽象的编码能力的时候，一味追求非线性反而会扭曲原来的直观信息．因此这篇文章针对机器翻译场景，提出在GRU单元内设置一个LAU(Linear Associate Unit)用来直接传递前一时刻编码输入，在此基础上改进NMT模型，思想简单效果却很好．   
为了更好地理解相关内容，首先得了解一下目前机器翻译的任务背景和最常用深度模型．简单的说，机器翻译的目标是将一种语言翻译成另外一种语言，而目前最常用的基于深度学习的结构称之为sequence2sequence的编解码器，其中编码器输入一个句子并且将其编码为呈现时间分布的隐藏变量，解码器接受编码器输出的隐藏变量输出解码的句子，编解码大多数用的是对序列输入输出效果较好的递归神经网络，另外为了动态的获取最佳隐藏变量，使用attention机制重新加权．其中递归神经网络的基本模块常用的是LSTM和GRU,该文章主要针对参数更少的GRU设计新的LAU单元.   
+ 先以概念的形式讲讲序列模型的思想:   
首先需要了解的是，序列模型的输入是一维向量，在自然语言中常常是词嵌入(Word Embedding)向量,即便这样的向量相对于bag-of-words的表达方式具备了一定的抽象性，但是对于不同模型参数下其表达方式仍然不是一成不变的，因此模型需要进一步通过参数去抽象化词嵌入向量并且希望它能表达这个序列的潜在意思．因此在序列模型中常用的就是用一个隐藏向量来传递含义，并且每个时刻根据输入向量和隐藏向量改变下输出给下一时刻变量，注意这里是针对GRU的序列模型，LSTM还含有状态变量．那么改变的方式怎么设计呢，其实就是设计合理的门限，在GRU中主要设计了更新和重置门限．   
其次通过公式以GRU为例量化上面的表达：  
定义序列在时刻t的隐藏变量：    
      <center>$h_t = (1-z_t)\odot h_{t-1}+z_t\odot \tilde{h_{t}}$</center>
其中 $\odot$ 表示点乘，$z_t$ 表示更新门限，$\tilde{h_{t}}$　表示激活候选也可理解为更新信号，其表达式如下：<center>$\tilde{h_{t}}=tanh(W_{xh}x_t+W_{hh}(r_t\odot h_{t-1}))$</center>
其中 $r_t$ 表示重置门限，重置门和更新门都取决于当前输入和上一时刻的隐藏变量,并通过sigmoid函数激  活，用tanh激活更新信号的原因是信号强度可增可减    
                    <center>$r_t=\sigma(W_{xr}x_t+W_{hr}h_{t-1})$   
                    $z_t=\sigma(W_{xz}x_t+W_{hz}h_{t-1})$</center>
通俗的讲，每个递归单元根据当前时刻的输出信号和上一时刻传递的隐藏信号调整更新和重置门，更新门控制更新比例，重置门控制需要更新的维度.  
接着引入LAU,直接承接上面的公式推导,引入新的隐藏变量表达式：<center>$h_t=((1-z_t)\odot h_{t-1}+z_t\odot \tilde{h_t}\odot (1-g_t)+g_t\odot H(x_t)$</center> 这里隐藏变量来源与三个部分，其一，直接来源于上一时刻的隐藏变量；其二，候选更新信号;其三，原始输入信号．注意对比原来GRU中候选更新信号是含有当前时刻原始输入的，只不过它传递给隐藏变量的值是经过非线性激活的．候选更新信号新的表达形式如下：<center>$\tilde{h_t}=tanh(f_t\odot(W_{xh}x_t)+r_t\odot(W_{hh}h_{t-1})$</center>
其中$f_t$和$r_t$分别控制输入信号和隐藏信号非线性特征提取的幅度，为了简便取$f_t=1-r_t$.现线性单元的表达式为$H(x_t)=W_xx_t$,系数向量$g_t$与$r_t$，$z_t$一样：<center>$g_t=\sigma(W_{xg}xt+W_{hg}h_t)$</center>
<center>![](https://naivescript.github.io/images/DEEPLAU.png)</center>
<center>图１</center>
+ 讲讲Attention的思想和结构   
整个机器翻译的模型的结构见图１.Attention model在机器翻译模型中应用广泛，这篇文章也大致讲解了其思路：    
alignment model $\alpha_{t,j}$ 表达了解码器第一层位于t-1的$s^{1}_{t-1}$ 与位置编码器最顶层位于j的 $h^{L_{enc}}_j$ 的匹配度，也可以理解为该单词在翻译时的重要程度.    
那么基于线性单元的思想，在匹配解码器隐藏信号的时候何不将该位置的目标单词的嵌入向量作为信号的一部分呢？于是文章有提出改进Attention的信号输入．具体地：以$c_j$表示Attention输出与编码器输出的线性加权:<center>$c_j=\sum_{t=1}^{t=Lx}\alpha_{t,j}h_t^{L_{enc}}$</center>
其中 <center>$\alpha_{t,j}=softmax(e_{t,j})$    
$e_{t,j}=v^{T}_{\alpha}\delta(W_as^{1}_{t-1}+U_ah_{h}^{L_{enc}}+W_yy_{t-1})$</center>
+ 总结：   
读完需要思考一个问题：信号的表达究竟是抽象还是直观，信号传递是非线性的还是线性的？这里难以回答，但是个人认为从根本上说由于存在感性因素所以生物信号是难以完全量化的，但是不妨碍我们去模拟它，从一些深度学习研究的角度也可以体现出信号的表达与传递是需要线性与非线性的结合，甚至需要结合一些规则和认知先验．




