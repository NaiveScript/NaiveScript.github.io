## Deep NMT with Linear Associate Unit    
引言与摘要：虽然深度学习得益于模型非线性近似能力在很多领域都取得极大的成功，但是与深度相关的话题一直也是学术界研究的重点．从sigmoid到Relu,从VGG到ResNet,相关研究也在围绕着如何训练和学习模型中展开．当网络已经具备抽象的编码能力的时候，一味追求非线性反而会扭曲原来的直观信息．因此这篇文章针对机器翻译场景，提出在GRU单元内设置一个LAU(Linear Associate Unit)用来直接传递前一时刻编码输入，在此基础上改进NMT模型，思想简单效果却很好．   
为了更好地理解相关内容，首先得了解一下目前机器翻译的任务背景和最常用深度模型．简单的说，机器翻译的目标是将一种语言翻译成另外一种语言，而目前最常用的基于深度学习的结构称之为sequence2sequence的编解码器，其中编码器输入一个句子并且将其编码为呈现时间分布的隐藏变量，解码器接受编码器输出的隐藏变量输出解码的句子，编解码大多数用的是对序列输入输出效果较好的递归神经网络，另外为了动态的获取最佳隐藏变量，使用attention机制重新加权．其中递归神经网络的基本模块常用的是LSTM和GRU,该文章主要针对参数更少的GRU设计新的LAU单元.   
<div>   

$$
J(\theta) = \frac 1 2 \sum_{i=1}^m (h_\theta(x^{(i)})-y^{(i)})^2
$$

</div>
你好似
